{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5abca005-9d8d-4d7e-82db-1febf62edada",
   "metadata": {},
   "source": [
    "모델의 전체 파라미터를 그대로 학습하는 대신에 대부분의 파라미터는 얼리고 일부의 파라미터만 사용함으로써 리소스를 절약\n",
    "\n",
    "라지 랭기지 모델을 fully traing할때 발생하는 catastrophic forgetting(새로운 task를 풀기위해 학습을 더 시키면 이전에 학습한것들 일부를 잊어버리는문제) 를 극복할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c84036d-203f-44b3-a122-93c6afd4e067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bca82e1-ebf8-4d31-8b1f-83f5ed9328e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(50257, 512, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "          (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('daily_tokenizer_0612')\n",
    "model = LlamaForCausalLM.from_pretrained('daily_llama_0612')\n",
    "\n",
    "model\n",
    "# 4개의 라마 어텐션 디코더 레이어가 있고, mlp layer가 3개가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc615eec-d736-4871-9643-ab92d46f16ee",
   "metadata": {},
   "source": [
    "# Efficient Fine Tuning : PEFT(PrintTrainableParameter)\n",
    "PEFT라는 패키지를 통해 실행할 수 있다. 여기서 Get PFT모델과 LoraConfig Task Type들을 import 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152ecc16-65d3-4532-be1d-602f27a354a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57fac484-97bb-4985-8d4d-24d329db88ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<TaskType.SEQ_CLS: 'SEQ_CLS'>,\n",
       " <TaskType.SEQ_2_SEQ_LM: 'SEQ_2_SEQ_LM'>,\n",
       " <TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
       " <TaskType.TOKEN_CLS: 'TOKEN_CLS'>,\n",
       " <TaskType.QUESTION_ANS: 'QUESTION_ANS'>,\n",
       " <TaskType.FEATURE_EXTRACTION: 'FEATURE_EXTRACTION'>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(TaskType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06b47b5-7a74-4193-a411-48e6aacfc9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(50257, 512, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=512, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=512, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "              (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "              (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                        inference_mode=False, #학습을 시킬것이기 때문에 False\n",
    "                        r=32, #r이 작을수록 trainable한 파라미터 수가 적어지고, 클수록 trainable한 파라미터가 커진다.\n",
    "                        lora_alpha=32, #sacling factor\n",
    "                        lora_dropout=0.1\n",
    "                        )\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.to(device)\n",
    "# PeftModelForCausalLM으로바뀌었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04129e5e-3241-4b42-8a7a-4a79f52b7902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262,144 || all params: 64,378,368 || trainable%: 0.40719267689420147\n"
     ]
    }
   ],
   "source": [
    "# PEFT METHOD를 통해서 학습 가능한 파라미터가 몇개인지 확인\n",
    "model.print_trainable_parameters() \n",
    "#6만개를 학습하고 나머지는 얼려놓는다. r의 크기가 크다면, trainable한 parameter가 커질컷이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ab2356-01d0-4ffd-b635-8171700ff1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['link', 'headline', 'category', 'short_description', 'authors', 'date'],\n",
       "        num_rows: 83878\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_cate = load_dataset('heegyu/news-category-balanced-top10')\n",
    "dataset_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c1dd60b-018a-4c0f-808b-f333a2a3722b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BUSINESS', 'ENTERTAINMENT', 'FOOD & DRINK', 'HEALTHY LIVING']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset label 정제\n",
    "categories = dataset_cate['train'].to_pandas().category.unique().tolist()\n",
    "categories.sort()\n",
    "categories = categories[:4]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "951d50f1-a548-4422-8734-ce486540543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['link', 'headline', 'category', 'short_description', 'authors', 'date'],\n",
       "        num_rows: 29026\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cate = dataset_cate.filter(lambda element: element['category'] in categories)\n",
    "dataset_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c44182c-3712-424d-be49-108a3118deba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'entertainment', 'food', 'healthy']\n",
      "{0: 'business', 1: 'entertainment', 2: 'food', 3: 'healthy'}\n",
      "{'business': 0, 'entertainment': 1, 'food': 2, 'healthy': 3}\n"
     ]
    }
   ],
   "source": [
    "# label과 integer사이를 mapping하는 딕셔너리\n",
    "categories = [x.split(' ')[0].lower() for x in categories] # 첫번째 단어만 소문자로\n",
    "int2label_cate = {i: categories[i] for i in range(len(categories))}\n",
    "label2int_cate = {int2label_cate[key]:key for key in int2label_cate}\n",
    "print(categories)\n",
    "print(int2label_cate)\n",
    "print(label2int_cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a02bbb-fbbd-46f5-b884-387e7faead50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e339dfbd659c4cc796844d3c2ce296b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['link', 'headline', 'category', 'short_description', 'authors', 'date', 'label'],\n",
       "        num_rows: 26123\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['link', 'headline', 'category', 'short_description', 'authors', 'date', 'label'],\n",
       "        num_rows: 2903\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_label(element):\n",
    "    category = element['category'].split(' ')[0].lower()\n",
    "    return {\"label\": label2int_cate[category], 'category': category}\n",
    "\n",
    "dataset_cate = dataset_cate.map(gen_label)\n",
    "dataset_cate = dataset_cate['train'].train_test_split(test_size=0.1)\n",
    "dataset_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbab3d08-3786-4909-8473-cfe5e0f04a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd5434421ad481296e424c680727b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prompt - article이 주어졌을때 category를 분류\n",
    "from datasets import DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "import random\n",
    "\n",
    "prompt_format1_cate = \"\"\"Given the article, what is the topic of the article? article: %s  answer: %s\"\"\"\n",
    "prompt_format2_cate = \"\"\"Determine the topic of the news article. article: %s answer: %s\"\"\"\n",
    "prompt_format3_cate = \"\"\"What is this article about? business/entertainment/food/healthy/parenting article: %s answer: %s\"\"\"\n",
    "\n",
    "prompts_cate = [prompt_format1_cate, prompt_format2_cate, prompt_format3_cate]\n",
    "\n",
    "def gen_prompt_cate(element):\n",
    "    prompt_format = prompts_cate[random.randint(0, len(prompts_cate)-1)] #prompt의 분배는 랜덤하게\n",
    "    return DatasetDict({'input': prompt_format%(element['headline'], int2label_cate[element['label']])}) #headline과 label로 채워준다.\n",
    "\n",
    "train_cate = dataset_cate['train'].map(gen_prompt_cate, remove_columns=dataset_cate['train'].column_names)\n",
    "train_dataset = train_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a045d31e-c13e-4978-838f-d7eb3fde5c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05099e920af24aef8ee8a43c52acc924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 26123\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize\n",
    "def tokenize(element):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    outputs = tokenizer(\n",
    "        element['input'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=True,\n",
    "        padding=True\n",
    "    )\n",
    "    return {\"input_ids\": outputs['input_ids']}\n",
    "\n",
    "context_length=128\n",
    "tokenized_datasets = train_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a85ac58-0c61-4eb4-8947-3436e69ca7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape : torch.Size([5, 49])\n",
      "attention_mask.shape : torch.Size([5, 49])\n",
      "labels.shape : torch.Size([5, 49])\n"
     ]
    }
   ],
   "source": [
    "#Data_Collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "out = data_collator([tokenized_datasets[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key}.shape : {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be520dc2-077c-4ffa-a0c7-670235b62ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82103\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Training Argument, Trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='peft_llama',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type='cosine',\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a8970c5-d7c7-4983-a61f-186ada72ac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82103\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:670: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='816' max='816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [816/816 02:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=816, training_loss=3.8046477074716605, metrics={'train_runtime': 176.5333, 'train_samples_per_second': 147.978, 'train_steps_per_second': 4.622, 'total_flos': 353064342478848.0, 'train_loss': 3.8046477074716605, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd085bdc-161b-48fa-814e-35171ba0a940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4088848958401da79be3c6a2397e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498849215fe6476bb3dbe09fa82a5a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids'],\n",
       "    num_rows: 2903\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"daily_tokenizer_0612\", padding_side='left')\n",
    "prompt_format1 = \"\"\"Given the article, what is the topic of the article? article: %s  answer:\"\"\"\n",
    "prompt_format2 = \"\"\"Determine the topic of the news article. article: %s answer:\"\"\"\n",
    "prompt_format3 = \"\"\"What is this article about? business/entertainment/food/healthy/parenting article: %s answer:\"\"\"\n",
    "\n",
    "prompts = [prompt_format1, prompt_format2, prompt_format3]\n",
    "\n",
    "def gen_valid_prompt_cate(element):\n",
    "    prompt_format = prompts[random.randint(0, len(prompts)-1)]\n",
    "    return DatasetDict({'input': prompt_format%(element['headline'])})\n",
    "\n",
    "valid_dataset = dataset_cate['test'].map(gen_valid_prompt_cate)\n",
    "\n",
    "context_length=128\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize, batched=True, remove_columns=['link', 'headline', 'category', 'short_description', 'authors', 'date', 'input']\n",
    ")\n",
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce67184f-63bb-42d4-864d-b597f64cf86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=4\n",
    "val_ds = valid_dataset.select(range(100))\n",
    "val_ds.set_format(type='torch')\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d3965b7-5ad5-4661-af8d-ca5df4c0d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "\n",
    "def acc(pred, label):\n",
    "    return torch.sum(torch.tensor(pred) == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "698980c6-942d-4670-ae36-e4f0aada6fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vall acc:  0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "val_losses = []\n",
    "val_acc = 0\n",
    "\n",
    "for step, batch in enumerate(tqdm(val_dl)):\n",
    "    label = batch['label']\n",
    "    input_id = batch['input_ids'].to(device)\n",
    "\n",
    "    pred = model.generate(input_ids=input_id, max_length=128) #input_ids가 다르다.\n",
    "    decoded_pred = tokenizer.batch_decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=False) #자연어 바꾸기\n",
    "    decoded_pred = [re.findall(\"answer: ([a-z]+)\", x)[0] if re.findall(\"answer: ([a-z]+)\", x) else 'none' for x in decoded_pred]\n",
    "    decoded_pred = [label2int_cate[x] if x in label2int_cate else -1 for x in decoded_pred] # integer label로 변환\n",
    "\n",
    "    val_acc += acc(decoded_pred, label)\n",
    "\n",
    "print(\"vall acc: \", val_acc/len(val_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4beb11dd-07ca-4cab-b87a-009fe83d9713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Given the article, what is the topic of the article? article: Holy Mother Of All That Is Good, 'Curb Your Enthusiasm' Is Officially Back  answer: entertainment  answer: entertainment  answer: entertainment  answer: entertainment  answer: entertainment  answer: entertainment  answer: entertainment, It Is A Child  answer: entertainment (VIDEO) answer: food/ity) answer: food, It Is A True answer: food answer: food answer: food answer: food answer: food answer: food, food, food\",\n",
       " \"Determine the topic of the news article. article: Chevron Chutzpah Knows No Bounds In Statements About Ecuadorians' Contamination Case answer: food answer: food answer: food answer: food answer: food answer: food answer: food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food\",\n",
       " 'Determine the topic of the news article. article: Global Drug Spending To Hit $1.4 Trillion In 2020 answer: food answer: food answer: food answer: food answer: food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer',\n",
       " 'Determine the topic of the news article. article: This Startup Offers Women An Amazing, Affordable And Thoughtful Perk answer: healthy answer: healthy answer: healthy food answer: food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food answer: food food food answer: food food answer: food food food answer']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일부의 작은 model을 쓰기 때문에 accuracy는 떨어진다.\n",
    "tokenizer.batch_decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b71ffadc-020f-431e-a1d6-2d513749168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('peft_llama_adapter__')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3cae24-7034-4e6a-baee-530e6640a70c",
   "metadata": {},
   "source": [
    "### 저장된 모델의 size 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0eb1041d-405b-43fb-b6fc-336813768df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0020065307617188"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.stat('peft_llama_adapter__/adapter_model.safetensors').st_size/(1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "070effb3-7237-41a2-899b-19d2f84f908f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244.58809661865234"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat('daily_llama_0612/model.safetensors').st_size/(1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a1c41-4ffe-4972-a280-99d7dfb66b14",
   "metadata": {},
   "source": [
    "모델의 size가 244정도 줄었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "882c9c3b-0180-4019-a90e-8e0c432f4c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244.58809661865234"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.stat('fake_detect_llama/model.safetensors').st_size/(1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0c026bd-3cae-457d-998f-70048688fa14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244.58809661865234"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    os.stat('llama_combined_0618/model.safetensors').st_size/(1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7bcdd01-4cd4-4b0e-bd84-92d70f6ab2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(50257, 512, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=512, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=512, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "              (up_proj): Linear(in_features=512, out_features=1376, bias=False)\n",
       "              (down_proj): Linear(in_features=1376, out_features=512, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "model_load = LlamaForCausalLM.from_pretrained('daily_llama_0612') #base model\n",
    "model_load = PeftModel.from_pretrained(model_load, 'peft_llama_adapter__')\n",
    "model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897245c-4777-4f56-ac51-fba4559bd8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
